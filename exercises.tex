\documentclass[]{book}

%These tell TeX which packages to use.
\usepackage{array,epsfig}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsxtra}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{color}
\usepackage{cleveref}
\usepackage{mathtools}

%Here I define some theorem styles and shortcut commands for symbols I use often
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{cor}{Corollary}
\newtheorem*{rmk}{Remark}
\newtheorem{lem}{Lemma}
\newtheorem*{joke}{Joke}
\newtheorem{ex}{Example}
\newtheorem*{soln}{Solution}
\newtheorem{prop}{Proposition}

\newcommand{\lra}{\longrightarrow}
\newcommand{\ra}{\rightarrow}
\newcommand{\surj}{\twoheadrightarrow}
\newcommand{\graph}{\mathrm{graph}} \newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\Z}{\bb{Z}} \newcommand{\Q}{\bb{Q}} \newcommand{\R}{\bb{R}}
\newcommand{\C}{\bb{C}} \newcommand{\N}{\bb{N}} \newcommand{\M}{\mathbf{M}}
\newcommand{\m}{\mathbf{m}} \newcommand{\MM}{\mathscr{M}}
\newcommand{\HH}{\mathscr{H}}
\newcommand{\Om}{\Omega}
\newcommand{\Ho}{\in\HH(\Om)}
\newcommand{\bd}{\partial}
\newcommand{\del}{\partial}
\newcommand{\bardel}{\overline\partial}
\newcommand{\textdf}[1]{\textbf{\textsf{#1}}\index{#1}}
\newcommand{\img}{\mathrm{img}}
\newcommand{\ip}[2]{\left\langle{#1},{#2}\right\rangle}
\newcommand{\inter}[1]{\mathrm{int}{#1}}
\newcommand{\exter}[1]{\mathrm{ext}{#1}} \newcommand{\cl}[1]{\mathrm{cl}{#1}}
\newcommand{\ds}{\displaystyle}
\newcommand{\vol}{\mathrm{vol}} \newcommand{\cnt}{\mathrm{ct}}
\newcommand{\osc}{\mathrm{osc}} \newcommand{\LL}{\mathbf{L}}
\newcommand{\UU}{\mathbf{U}} \newcommand{\support}{\mathrm{support}}
\newcommand{\AND}{\;\wedge\;}
\newcommand{\OR}{\;\vee\;}
\newcommand{\Oset}{\varnothing}
\newcommand{\st}{\ni}
\newcommand{\wh}{\widehat}
\newcommand{\XX}{\mathbf{X}}
\newcommand{\YY}{\mathbf{Y}}
\newcommand{\TT}{\mathcal{T}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min} \DeclareMathOperator*{\Cov}{Cov}
\DeclareMathOperator*{\EPE}{EPE}
\DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Bias}{Bias}


%Pagination stuff.
\setlength{\topmargin}{-.3 in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textheight}{9.in}
\setlength{\textwidth}{6.5in}
\pagestyle{empty}



\begin{document}


\begin{center}
	{\Large Math 3220-1 \hspace{0.5cm} HW 1}\\
	\textbf{NAME}\\ %You should put your name here
	Due: DATE %You should write the date here.
\end{center}

\vspace{0.2 cm}


\subsection*{Exercises for Section 2}

\begin{enumerate}
	\item\label{ex:k-classes} Suppose each of $K$-classes has an associated
	target $t_k$, which is a vector of all zeros, except a one in the $k$th
	position. Show that classifying to the largest element of $\hat{y}$ amounts
	to choosing the closest target, $\min_k\|t_k-\hat{y}\|$, if the elements of
	$\hat{y}$ sum to one.
	\begin{soln}
		\newcommand{\normone}[1]{\sum_{i\ne #1}|\hat{y}_i|+|1-\hat{y}_{#1}|} Let
		$k^*=\argmax_k \hat{y}_k$ and suppose that there is $k'\le k^*$ such
		that $\|t_{k'}-\hat{y}\| < \|t_{k^*}-\hat{y}\|$.
		\begin{itemize}
			\item $\ell_1$ norm. It holds that
			      $\|t_k-\hat{y}\|_1=\sum_i|t_{k,i}-\hat{y}_i|=\sum_{i\ne
					      k}|\hat{y}_i|+|1-\hat{y}_k|$. Hence, we get
			      \begin{equation}\label{2.1-inequality}
				      \normone{k'} < \normone{k^*}\Rightarrow |\hat{y}_{k^*}|-|1-\hat{y}_{k^*}|
				      < |\hat{y}_{k'}|-|1-\hat{y}_{k'}|.
			      \end{equation}
			      But the function $f(y)=|y|-|1-y|$ is increasing in $[0,1]$
			      hence~\Cref{2.1-inequality} implies that
			      $\hat{y}_{k^*}<\hat{y}_{k'}$, reaching a contradiction.
			\item $\ell_2$ norm. Similarly, we get that
			      $\hat{y}_{k^*}(1-\hat{y}_{k^*})<\hat{y}_{k'}(1-\hat{y}_{k'})$
			      and since the function $f(y)=y(1-y)$ is increasing in $[0,1]$,
			      we get that $\hat{y}_{k^*}<\hat{y}_{k'}$, reaching a
			      contradiction.
		\end{itemize}
	\end{soln}

	\item\label{ex:exact-distribution} Show how to compute the Bayes decision
	boundary for the simulation example in Figure 2.5.
	\begin{soln}
		If we know the exact probability distribution $\Pr[G,X]$,
		$X\in\mathbb{R}^p$, $G\in \mathcal{G}=\{B,O\}$, then we can probably
		also derive $f(X)=\Pr[B|X]=\Pr[B,X]/\Pr[X]$, namely the probability that
		$X$ maps to blue in reality. This assume that we also know $\Pr[X]$
		which is not necessary. Of course, $\Pr[O|X]=1-\Pr[B|X]$. So now, all we
		have to do is to check for each $x\in\mathbb{R}^p$, whether $f(x)>1/2$.
		For the case where $x\in\mathbb{R}$, this is trivial. We simply solve
		the equation $f(x)=1/2$. This also hold in general. So the points (in
		$\mathbb{R}$), the line (in $\mathbb{R}^2$), and the $(p-1)$-dimensional
		hyperplane (in $\mathbb{R}^p$), is the solution to the equation
		$f(x)=\Pr[B|X]=1/2$. See Figure~\ref{fig:exercise2.2} for another
		example.

		\begin{figure}[ht]
			\label{fig:exercise2.2}
			\includegraphics[width=8cm]{plots/ex22.png}
			\centering
			\caption{In this example we have computed the Bayes decision
			boundary when $X\sim U(0,1)^2$ and
			$\Pr[Y=\text{red}|X]=X_1^{1/10}X_2$. Therefore, the line is the
			solution to the equation $X_1^{1/10}X_2=1/2$.}
		\end{figure}
	\end{soln}

	\item\label{ex:eq2.24} Derive equation 2.24. Consider $N$ data points
	uniformly sampled in a $p$-dimensional unit ball centered at the origin.
	Show that the median distance from the origin to the closest data point is
	given by the expression $$d(p,N)=\left(1-\frac{1}{2}^{1/N}\right)^{1/p}.$$
	\begin{soln}
		We start with the cumulative distribution function (CDF) of the distance
		of a random point from the origin. The volume of a $p$-dimensional ball
		of radius $d$ is $V_p(d)=c_pd^p$, where $c_p$ is a value that does not
		depend on $d$. Therefore,
		\begin{equation}\tag{First trick to remember}
			F_D(d)=\Pr[D\le d]=\frac{V_p(d)}{V_p(1)}=d^p.
		\end{equation}
		Now it is useful to compute the CDF of the distance of the closest point
		$C=\min_{i\in[N]} D_i$. We have that
		\begin{equation}
			\begin{split}
				F_C(d) &= \Pr[C\le d] \\
				&= 1-\Pr[C\ge d] \\
				&= 1-\Pr\left[\min_{i\in[N]} D_i \ge d\right] \\
				&= 1-\Pr[\forall i\in[N], D_i \ge d] \\
				&= 1-\prod_{i\in[N]}\Pr[D_i \ge d] \\
				&= 1-\Pr[D \ge d]^N \\
				&= 1-(1-\Pr[D \le d])^N \\
				&= 1-(1-d^p)^N.
			\end{split}
		\end{equation}
		By definition, the median $m$ is defined as $F_C(m)=1/2$. Hence, we get
		that $(1-m^p)^N=1/2$ and solving for $m$, we get
		\[m=\left(1-\frac{1}{2}^{1/N}\right)^{1/p}.\]
	\end{soln}

	\item\label{ex:multi-normal} Consider inputs drawn from a spherical
	multinormal distribution $X\sim N(0,\mathbf{I}_p)$. The squared distance
	from any sample point to the origin has a $\chi_p^2$ distribution with mean
	$p$. Consider a prediction point $x_0$ drawn from this distribution, and let
	$a=x_0/\|x_0\|$ be an associated unit vector. Let $z_i=a^Tx_i$ be the
	projection of each of the training points on this direction.

	Show that the $z_i$ are distributed according to $N(0,1)$ with expected
	squared distance from the origin $1$, while the target point has expected
	squared distance $p$ from the origin.

	\begin{soln}
		We use the fact that for any $a\in\mathbb{R}^p$, if $x\sim
			N(0,\mathbf{I}_p)$, then $a^Tx\sim
			N\left(\sum_ja_j\mu_j,\sum_ja_j^2\sigma_j^2\right)$, where
		$\mu_j=E(x_j)$ and $\sigma_j=V(x_j)$ and $j\in[p]$. Since
		$\sigma_j=1$ and $\mu_j=0$, we get that $a^Tx\sim
			N\left(0,\sum_ja_j^2\right)$. Given that $\|a\|$ is a unit vector,
		we get that $a^Tx\sim N\left(0,1\right)$. Hence
		$|z|=|a^Tx|\sim\chi_1^2$ and $E[|z|]=1$.
	\end{soln}

	\item\label{ex:eq2.27} Suppose that we know that the true relationship
	between $Y$ and $X$ is linear,
	\begin{equation}
		Y=X^T\beta+\varepsilon, \tag{2.26}
	\end{equation}
	where $\varepsilon\sim N(0,\sigma^2)$ and we fit the model by least squares
	to the training data. For an arbitrary test point $x_0$, we have $\hat
		y_0=x_0^T\hat\beta$, which can be written as $\hat y_0=x_0^T\beta +
		\sum_{i-1}^N\ell_i(x_0)\varepsilon_i$, where $\ell_i(x_0)$ is the $i$th
	element of $\XX(\XX^T\XX)^{-1}x_0$. Show that
	\[
		\EPE(x_0)=\sigma^2+E_\TT x_0^T(\XX^T\XX)^{-1}x_0\sigma^2+0^2. \tag{2.27},\]
	where you can use the fact that for any $\XX$,
	\[\Cov[\hat \beta]=(\XX^T\XX)^{-1}\sigma^2. \label{eq:cov}\tag{3.8}\]
	Additionally, suppose $N$ is large and $\TT$ were selected at random.
	Assuming $E(X)=0$, then $\XX^T\XX\to N\Cov(X)$. Show that
	\[E_{x_0}\EPE(x_0)\approx\sigma^2(p/N)+\sigma^2. \tag{2.28}\]

	\begin{soln}
		{
			\newcommand{\Exp}{E_{\TT,\varepsilon}}
			In the first question, the test point $x_0$ is arbitrary and not sampled
			from the distribution. Thus the randomness is only over:
			\begin{itemize}
				\item the samples $\TT$,
				\item the error $\varepsilon$.
			\end{itemize}
			In the second part, we also sample $x_0$ and hence we consider the
			expectation of $\EPE(x_0)$.
			\begin{enumerate}
				\item We start by showing that the expected prediction error equals
				      the sum of the variance of the system, the variance of the model and
				      the squared bias of the model:
				      \begin{equation*}
					      \begin{split}
						      \EPE(x_0) &= \Exp[(y_0-\hat y_0)^2] \\
						      &= \Exp[y_0^2-2y_0\hat y_0 + \hat y_0^2] \\
						      &= \Exp[y_0^2]-2\Exp[y_0\hat y_0] + \Exp[\hat y_0^2] \\
						      &= \Exp[y_0^2]-2x_0^T\beta\Exp[\hat y_0] + \Exp[\hat y_0^2] \\
						      &= \Exp[y_0^2]\boxed{-\Exp[y_0]^2+\Exp[y_0]^2}-2x_0^T\beta\Exp[\hat y_0] + \Exp[\hat y_0^2] - \boxed{\Exp[\hat y_0]^2 + \Exp[\hat y_0]^2} \\
						      &= \Var[y_0] + \Var[\hat y_0] + \left(\Exp[\hat y_0]-x_0^T\beta\right)^2,
					      \end{split}
				      \end{equation*}
				      where the third line follows from the fact that
				      \[\Exp[y_0\hat y_0]=\Exp[(x_0^T\beta+\varepsilon)\hat
						      y_0]=x_0^T\beta\Exp[\hat y_0]+\Exp[\varepsilon\hat y_0]\]
				      and $\Exp[\varepsilon\hat y_0]=\Exp[\varepsilon]\Exp[\hat
						      y_0]=0$, since $\varepsilon$ is independent from $\hat
					      y_0$.
				      Now, we have that $\Var[y_0]=\sigma^2$. Moreover,
				      \begin{equation}
					      \begin{split}
						      \Exp[\hat y_0] &= \Exp\left[x_0^T\beta + \sum_{i-1}^N\ell_i(x_0)\varepsilon_i\right] \\
							  &= x_0^T\beta + \sum_{i-1}^N\Exp[\ell_i(x_0)\varepsilon_i] \\
							  &= x_0^T\beta + \sum_{i-1}^N\Exp[\ell_i(x_0)]\Exp[\varepsilon_i] \\
							  &= x_0^T\beta,
					      \end{split}
				      \end{equation}
					  since $\varepsilon_i$ is independent of $x_0$ and $\XX$.
					  Hence, $\Bias(\hat y_0)=\left(\Exp[\hat
					  y_0]-x_0^T\beta\right)=0$. Last, we want to calculate the
					  variance of our prediction $\Var[\hat y_0]$. We have 
					  \begin{equation}
						  \begin{split}
							  \Var[\hat y_0] &= \Var[x_0^T\hat\beta] \\
							  &= x_0^T\Cov[\hat\beta]x_0 \\
							  &= x_0^T(\XX^T\XX)^{-1}\sigma^2x_0,
						  \end{split}
					  \end{equation}
					  where the last line comes from~\cref{eq:cov}.
					  
			\end{enumerate}
		}
	\end{soln}
\end{enumerate}



\end{document}


