\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}	% Para caracteres en espa√±ol
\usepackage{amsmath,amsthm,amsfonts,amssymb,amscd}
\usepackage{multirow,booktabs}
\usepackage[table]{xcolor}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{enumitem}
\usepackage{fancyhdr}
\usepackage{mathrsfs}
\usepackage{wrapfig}
\usepackage{setspace}
\usepackage{calc}
\usepackage{multicol}
\usepackage{cancel}
\usepackage[retainorgcmds]{IEEEtrantools}
\usepackage[margin=3cm]{geometry}
\usepackage{amsmath}
\newlength{\tabcont}
\setlength{\parindent}{0.0in}
\setlength{\parskip}{0.05in}
\usepackage{empheq}
\usepackage{framed}
\usepackage[most]{tcolorbox}
\usepackage{xcolor}
\usepackage{pagecolor}

% \pagecolor{black}
% \color{white}

\colorlet{shadecolor}{blue!40}
\parindent 0in
\parskip 12pt
\geometry{margin=1in, headsep=0.25in}
\theoremstyle{definition}
\newtheorem{defn}{Definition}
\newtheorem{reg}{Rule}
\newtheorem{exer}{Exercise}
\newtheorem{note}{Note}

\newcommand{\XX}{\mathbf{X}}
\newcommand{\YY}{\mathbf{Y}}
\newcommand{\yy}{\mathbf{y}}
\newcommand{\II}{\mathbf{I}}
\newcommand{\IID}{\textsf{IID}}
\newcommand{\Normal}[2]{\ensuremath{\mathcal N (#1, #2)}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min} \DeclareMathOperator*{\Cov}{Cov}
\DeclareMathOperator*{\EPE}{EPE} \DeclareMathOperator*{\Var}{Var}
\DeclareMathOperator*{\Bias}{Bias} \DeclareMathOperator*{\tr}{trace}
\DeclareMathOperator*{\RSS}{RSS} \DeclareMathOperator*{\WRSS}{WRSS}
\DeclareMathOperator*{\MSE}{MSE} \DeclareMathOperator*{\diag}{diag}

\begin{document}
\title{Chapter 3 Review Notes}

\thispagestyle{empty}

\begin{center}
	{\LARGE \bf Chapter 3 Lecture Notes}\\
	{\large Elements of Statistical Learning}
\end{center}

\section{Means and Variances}

\begin{shaded}
	\textbf{Important Properties of Estimators} \newline
	Let $x_1,\ldots,x_N$ be i.i.d. with mean $\mu$ and variance $\sigma^2$. Let
	$\hat \mu=E_i[x_i]$ and $\hat \sigma^2 = \frac{N}{N-1}E_i(x_i-\hat \mu)^2$ be
	the estimated mean and estimated variance respectively. Then $E[\hat\mu]=\mu$
	and $E[\hat\sigma^2]=\sigma^2$.
\end{shaded}

First, it is easy to show that $E[\hat \mu]=\mu$:
\begin{equation}
	\begin{split}
		E[\hat\mu] &= E[E_i[x_i]] \\
		&= E_i[E[x_i]] \\
		&= E_i[\mu] \\
		&= \mu.
	\end{split}
\end{equation}

Next, we know that
\begin{itemize}
	\item $\Var(c x)=c^2\Var(x)$
	\item $\Var(\sum x_i)=n\Var(x_i)=n\sigma^2$, since they are i.i.d..
	\item The above two, give us that $\Var[\hat\mu]=\frac{\sigma^2}{n}$. This
	      makes sense, the more data we have the closer we get to the mean, and so the
	      variance becomes smaller.
\end{itemize}

Using the above property, we can compute the expected estimated variance. First,
notice that
\begin{equation}
	\begin{split}
		E_i(x_i-\hat\mu)^2 &= E_i(x_i^2-2x_i\hat\mu+\hat\mu^2) \\
		&= E_i(x_i^2) - 2\hat\mu E_i(x_i) + \hat\mu^2 \\
		&= E_i(x_i^2) - \hat\mu^2.
	\end{split}
\end{equation}

Therefore, we compute $E(\hat\sigma^2)$ as follows:
\begin{equation}
	\begin{split}
		E(\hat\sigma^2) &= \frac{N}{N-1}E(E_i(x_i-\hat \mu)^2) \\
		&= \frac{N}{N-1}E(E_i(x_i^2) - \hat\mu^2) \\
		&= \frac{N}{N-1}\left[E(E_i(x_i^2)) - E(\hat\mu^2)\right] \\
		&= \frac{N}{N-1}\left[E_i(E(x_i^2)) - E(\hat\mu^2)\right] \\
		&= \frac{N}{N-1}\left[E_i(\sigma^2-\mu^2) - E(\hat\mu^2)\right] \\
		&= \frac{N}{N-1}\left[\sigma^2-\mu^2 - \Var\hat\mu+(E\hat\mu)^2\right] \\
		&= \frac{N}{N-1}\left[\sigma^2-\mu^2 - \sigma^2/N+\mu^2\right] \\
		&= \frac{N}{N-1}\frac{N-1}{N}\sigma^2 = \sigma^2.
	\end{split}
\end{equation}

\section{Estimated Values in Linear Regression}

\begin{shaded}
	\textbf{Important Properties of Estimators} \newline
	Suppose that the regression function $E[Y|X]=f(X)$ and that $\Var
		Y=\sigma^2$. Suppose also that $x_i$ are fixed, not random, and the only
	randomness is on the $y_i$. We compute the least squares as $\hat\beta =
		(\XX^T\XX)^{-1}\XX^T\yy$ and the estimated variance as
	$\hat\sigma^2=\frac{N}{N-p-1}E_i(y_i-\hat y_i)^2$. Then
	$\Var[\hat\beta]=(\XX^T\XX)^{-1}\sigma^2$ and $E[\hat\sigma^2]=\sigma^2$.
\end{shaded}

We first compute $\Var[\hat\beta]$. We have
\begin{equation}
	\begin{split}
		\Var[\hat\beta] &= \Var[(\XX^T\XX)^{-1}\XX^T\yy] \\
		&= (\XX^T\XX)^{-1}\XX^T \Var[\yy] \XX(\XX^T\XX)^{-1} \\
		&= (\XX^T\XX)^{-1}\XX^T \II_p\sigma^2 \XX(\XX^T\XX)^{-1} \\
		&= (\XX^T\XX)^{-1}\sigma^2.
	\end{split}
\end{equation}


\section{Important Distributions}

\subsection{Chi-squared}
Let $x_1,\ldots,x_n$ be \IID~ standard normal random variables
$x_i\sim\mathcal{N}(0,1)$ and let $\|x\|_2=nE_ix_i^2=\sum_i x_i^2$. Then
$$\|x\|_2\sim\chi_n^2.$$

Of course $\chi_n^2$ is always positive and drops similarly to how the normal
drops on its right.

Clearly, the same holds no matter whether the vector is free or bound. Hence, if
$x\sim\mathcal{N}(\mu,I_n)$, then $$\|x\|_2=\sum(x_i-\mu)^2\sim\chi_n^2$$ again.

Additionally, Cochran's theorem states that
$nE_i(x_i-E_ix_i)^2=\sum_i(x_i-\hat\mu)^2\sim\chi_{n-1}^2$. So, if we instead of
the real mean we use the estimated mean, we lose one degree of freedom.

Also, $(n-1)\hat\sigma^2\sim \sigma^2\chi_{n-1}^2$.

\subsection{$t$-distribution}
As before, let $x_1,\ldots,x_n\sim\Normal\mu{\sigma^2}$. We saw above that the
sample variance (a.k.a. unbiased variance estimation) is
$\hat\sigma^2=\frac{N}{N-1}E_i(x_i-\hat\mu)^2$. It holds that
\begin{equation}
	\frac{\hat\mu-\mu}{\sigma/\sqrt n}~\sim\Normal{0}{1}.
\end{equation}
However, if instead we use the sample variance, we get
\begin{equation}
	\frac{\hat\mu-\mu}{\hat\sigma/\sqrt n}~\sim t_{n-1}.
\end{equation}
Observe that both of the above quantities have distributions that do not depend
neither on $\mu$ nor on $\sigma$. However, specifically the second one has only
one unknown, the mean $\mu$. So we can use this distribution to derive
confidence intervals for $\mu$.

For example, we can make the null hypothesis that $\mu=\mu^*$ for some $\mu^*$.
If this is the case, then we expect the expression
\[\frac{\hat\mu-\mu^*}{\hat\sigma\sqrt{n}}\] to follow the
$t_{n-1}$-distribution which is almost the same as the normal distribution. But
not that we know all parameters of this expression so we can easily evaluate it.
If we get a value that is close to 0 then we accept the hypothesis, otherwise we
reject it.

\begin{shaded}
	\textbf{Hypothesis testing using the $t$-distribution} \newline
    As the sample size $n$ increases, the two distributions $\Normal 0 1$ and
    $t_{n-1}$ look very alike and $t_\infty=\Normal 0 1$. Especially the
    difference between their tail quantiles becomes negligible in $n$.
    This suggests an easy way to verify the null hypothesis that $\mu=0$. Simply
    take the quantity $\frac{\hat\mu}{\hat\sigma^2/\sqrt n}$ and check whether
    it is far from zero. The further from zero it is, the more likely that the
    null hypothesis does not hold and should be rejected.
\end{shaded}


\begin{defn}[$t$-distribution]
	Let $Z,V$ independent random variables, such that $Z\sim\Normal 0 1$ and
	$V\sim\chi_v^2$. Then
	\[T=\frac{Z}{\sqrt{V/v}}\sim t_v\]
\end{defn}

Student's $t$ looks like a normal distribution but pushed down.

\subsection{$F$-distribution}
My first observation is that the $F$ distribution is to the $t$-distribution
what the $\chi$-distribution is to the normal distribution. The $F$-distribution
has two degrees of freedom $d_1,d_2$ and we write $F_{d_1,d_2}$.


\begin{shaded}
	\textbf{Property of $\chi^2$ and $F$} \newline
	Suppose $X$ has a Student's $t$-distribution with degree of freedom $v$;
	i.e., $X\sim t_v$. Then $X^2\sim F_{1,v}$.
\end{shaded}

\begin{defn}
    Suppose $S_1\sim\chi_{d_1}^2$ and $S_2\sim\chi_{d_2}^2$ are two independent
    random variables. Then 
    \[X=\frac{S_1/d_1}{S_2/d_2}\sim F_{d_1,d_2}.\]
\end{defn}
So it is the ratio of two independent appropriately scaled $\chi^2$ distributions.

Similarly to the chi-squared distribution, the $F$-distribution is always positive
and has a similar shape as the chi-squared.

\subsection{$Z$-score}
Let $X$ be a random variable with mean $\mu$ and variance $\sigma$. Let
$x\leftarrow X$, be a sample from this distribution. Then the $Z$-score of $x$
is the distance of $x$ from the mean, measured in standard deviations:
\[ z=\frac{x-\mu}{\sigma}.\]

Notice that if $x\sim\Normal{\mu}{\sigma^2}$, then $z\sim\Normal 0 1$; i.e., it
follows the standard normal distribution. This is called \emph{normalization} in
general. The goal is to use our samples to arrive to a \emph{pivotal quantity},
meaning a quantity whose distribution is independent of the parameters of the
real distribution.

\paragraph{Example.} Suppose that $\hat x=\hat\mu=E_i x_i$ is the average (sample mean,
mean estimation) of the $x_i$s. We saw earlier that $E[\hat{x}]=\mu$. Moreover,
$\hat\sigma=\frac{N}{N-1}E_i(x_i-\hat\mu)^2$ is the sample variance such that
$E[\hat\sigma^2]=\sigma^2$. Then the Z-score of $\hat x$, is 
\[z=\frac{\hat x-E[\hat x]}{\hat\sigma/\sqrt{N}}.\]
This follows the distribution $t_{N-1}$ as we saw.

\section{Applications to 3.2}
We have already seen that
\[\Var[\hat\beta]=(\XX^T\XX)^{-1}\sigma^2.\]

Moreover, similarly to how we computed the unbiased variance estimator above for
one dimension, here we have $p+1$ dimensions, and hence,
\[\hat\sigma^2=\frac{N}{N-p-1}E_i(\hat y_i-y_i)^2.\]

As before, $E[\hat\sigma^2]=\sigma^2$.

Now, let's additionally assume that the real model is linear; i.e.
$E[Y|X]=X^T\beta$ and $Y=X^T\beta+\varepsilon$, where
$\varepsilon\sim\Normal{0}{\sigma^2}$. 

In this case, our beta estimator $\hat\beta$ is also normal:
\[\hat\beta\sim\Normal{\beta}{(\XX^T\XX)^{-1}\sigma^2}.\]

Moreover, $(N-p-1)\hat\sigma^2\sim\sigma^2\chi^2_{N-p-1}$ similarly to what we 
had above but for more dimensions.

Also, $\Cov(\hat\beta,\hat\sigma^2)=0$.
 
\end{document}